{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Basic causal discovery with DoDiscover using the PC algorithm\n\nWe will simulate some observational data from a Structural Causal Model (SCM) and\ndemonstrate how we will use the PC algorithm.\n\nThe PC algorithm works on observational data when there are no unobserved latent\nconfounders. That means for any observed set of variables, there is no common causes\nthat are unobserved. In other words, all exogenous variables then are assumed to be\nindependent.\n\nIn this example, we will introduce the main abstractions and concepts used in\ndodiscover for causal discovery:\n\n- learner: Any causal discovery algorithm that has a similar scikit-learn API.\n- context: Causal assumptions.\n\n.. currentmodule:: dodiscover\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Authors: Adam Li <adam2392@gmail.com>\n\nLicense: BSD (3-clause)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport networkx as nx\nfrom scipy import stats\nfrom pywhy_graphs.viz import draw\nfrom dodiscover.ci import GSquareCITest, Oracle\nfrom dodiscover import PC, make_context\nimport pandas as pd\nfrom dowhy import gcm\nfrom dowhy.gcm.util.general import set_random_seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate some data\nFirst we will simulate data, starting from a Structural Causal Model (SCM).\nThis will then induce a causal graph, which we can visualize. Due to the\nMarkov assumption, then we can use d-separation to examine which variables\nare conditionally independent.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# set a random seed to make example reproducible\nseed = 12345\nrng = np.random.RandomState(seed=seed)\n\n\nclass MyCustomModel(gcm.PredictionModel):\n    def __init__(self, coefficient):\n        self.coefficient = coefficient\n\n    def fit(self, X, Y):\n        # Nothing to fit here, since we know the ground truth.\n        pass\n\n    def predict(self, X):\n        return self.coefficient * X\n\n    def clone(self):\n        # We don't really need this actually.\n        return MyCustomModel(self.coefficient)\n\n\n# set a random seed to make example reproducible\nset_random_seed(1234)\n\n# construct a causal graph that will result in\n# x -> y <- z -> w\nG = nx.DiGraph([(\"x\", \"y\"), (\"z\", \"y\"), (\"z\", \"w\")])\n\ncausal_model = gcm.ProbabilisticCausalModel(G)\ncausal_model.set_causal_mechanism(\"x\", gcm.ScipyDistribution(stats.binom, p=0.5, n=1))\ncausal_model.set_causal_mechanism(\"z\", gcm.ScipyDistribution(stats.binom, p=0.9, n=1))\ncausal_model.set_causal_mechanism(\n    \"y\",\n    gcm.AdditiveNoiseModel(\n        prediction_model=MyCustomModel(1),\n        noise_model=gcm.ScipyDistribution(stats.binom, p=0.8, n=1),\n    ),\n)\ncausal_model.set_causal_mechanism(\n    \"w\",\n    gcm.AdditiveNoiseModel(\n        prediction_model=MyCustomModel(1),\n        noise_model=gcm.ScipyDistribution(stats.binom, p=0.5, n=1),\n    ),\n)\n\n# Fit here would not really fit parameters, since we don't do anything in the fit method.\n# Here, we only need this to ensure that each FCM has the correct local hash (i.e., we\n# get an inconsistency error if we would modify the graph afterwards without updating\n# the FCMs). Having an empty data set is a small workaround, since all models are\n# pre-defined.\ngcm.fit(causal_model, pd.DataFrame(columns=[\"x\", \"y\", \"z\", \"w\"]))\n\n# sample the observational data\ndata = gcm.draw_samples(causal_model, num_samples=500)\n\nprint(data.head())\nprint(pd.Series({col: data[col].unique() for col in data}))\ndraw(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate some conditional independence tests\nTo run a constraint-based structure learning algorithm such\nas the PC algorithm, we need a way to test for conditional\nindependence (CI) constraints. There are various ways we can evaluate\nthe algorithm.\n\nIf we are applying the algorithm on real data, we would want to\nuse the CI test that best suits the data. Note that because of\nfinite sample sizes, any CI test is bound to make some errors, which\nresults in incorrect orientations and edges in the learned graph.\n\nIf we are interested in evaluating how the structure learning algorithm\nworks in an ideal setting, we can use an oracle, which is imbued with the\nground-truth graph, which can query all the d-separation statements needed.\nThis can help one determine in a simulation setting, what is the best case\ngraph the PC algorithm can learn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "oracle = Oracle(G)\nci_estimator = GSquareCITest(data_type=\"discrete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the context\nIn PyWhy, we introduce the :class:`context.Context` class, which should be a departure from\n\"data-first causal discovery,\" where users provide data as the primary input to a\ndiscovery algorithm. This problem with this approach is that it encourages novice\nusers to see the algorithm as a philosopher's stone that converts data to causal\nrelationships. With this mindset, users tend to surrender the task of providing\ndomain-specific assumptions that enable identifiability to the algorithm. In\ncontrast, PyWhy's key strength is how it guides users to specifying domain\nassumptions up front (in the form of a DAG) before the data is added, and\nthen addresses identifiability given those assumptions and data. In this sense,\nthe Context class houses both data, apriori assumptions and other relevant data\nthat may be used in downstream structure learning algorithms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "context = make_context().variables(data=data).build()\n\n# Alternatively, one could say specify some fixed edges.\n# Note that when specifying fixed edges, the resulting graph that is\n# learned is not necessarily a \"pure CPDAG\". In that, there are more\n# constraints than just the conditional independences. Therefore, one\n# should use caution when specifying fixed edges if they are interested\n# in leveraging ID or estimation algorithms that assume the learned\n# structure is a \"pure CPDAG\".\n\n# .. code-block::Python\n#   included_edges = nx.Graph([('x', 'y')])\n#   context = make_context().edges(include=included_edges).build()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run structure learning algorithm\nNow we are ready to run the PC algorithm. First, we will show the output of\nthe oracle, which is the best case scenario the PC algorithm can learn given\nan infinite amount of data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pc = PC(ci_estimator=oracle)\npc.fit(data, context)\n\n# The resulting completely partially directed acyclic graph (CPDAG) that is learned\n# is a \"Markov equivalence class\", which encodes all the conditional dependences that\n# were learned from the data.\ngraph = pc.graph_\n\ndot_graph = draw(graph)\ndot_graph.render(outfile=\"oracle_cpdag.png\", view=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will show the output given a real CI test, which performs CI hypothesis testing\nto determine CI in the data. Due to finite data and the presence of noise, there is\nalways a possibility that the CI test makes a mistake.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pc = PC(ci_estimator=ci_estimator)\npc.fit(data, context)\n\n# The resulting completely partially directed acyclic graph (CPDAG) that is learned\n# is a \"Markov equivalence class\", which encodes all the conditional dependences that\n# were learned from the data. Note here, because the CI test fails to find the\n# dependency between 'z' and 'y', then we fail to also orient the corresponding\n# collider in the data. This illustrates the dependency of constraint-based\n# structure learning algorithms on the implicit Type I/II error in hypothesis tests.\ngraph = pc.graph_\n\ndot_graph = draw(graph)\ndot_graph.render(outfile=\"ci_cpdag.png\", view=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Causal discovery with interventional data - Sachs dataset\n\nWe will analyze the Sachs dataset :footcite:`sachsdataset2005` and reproduce analyses\nfrom the Supplemental Figure 8 in :footcite:`Jaber2020causal` demonstrating the\nusage of the :class:`dodiscover.constraint.PsiFCI` algorithm for learning causal graphs\nfrom observational and/or interventional data.\n\nThe Sachs dataset is a famous dataset in causal discovery because of its real-life\napplicability and access to experimental data that analyzed the causal network of\nprotein signaling pathways. We will analyze the preprocessed interventional dataset,\nwhich we download using the package [pooch](https://www.fatiando.org/pooch/latest/).\nThe preprocessed dataset consists of categorical features, so we will use the\n:class:`dodiscover.ci.GSquareCITest` for testing conditional independence and\ninvariances of the conditional distributions across experimental conditions.\nThere are a total of 6 experimental conditions represented by the ``INT`` column.\n\n.. currentmodule:: dodiscover\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Authors: Adam Li <adam2392@gmail.com>\n\nLicense: BSD (3-clause)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pywhy_graphs.viz import draw\nfrom dodiscover.ci import GSquareCITest\nfrom dodiscover import PsiFCI, Context, make_context, InterventionalContextBuilder\n\nimport pandas as pd\nimport bnlearn\n\nimport pooch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pull in the Sachs Dataset\nThe Sachs dataset is a famous dataset in causal discovery because of its real-life\napplicability and access to experimental data that analyzed the causal network of\n11 proteins using knockouts and spikings :footcite:`sachsdataset2005`. The pathways\nfor those proteins are already known, so it is an ideal dataset for benchmarking\ncausal discovery algorithms.\n\nWe will download a preprocessed version of the dataset from the following\nurl: https://www.bnlearn.com/book-crc/code/sachs.interventional.txt.gz\n\nRef: https://erdogant.github.io/bnlearn/pages/html/bnlearn.bnlearn.html#bnlearn.bnlearn.import_example  # noqa\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# use pooch to download robustly from a url\nurl = \"https://www.bnlearn.com/book-crc/code/sachs.interventional.txt.gz\"\nfile_path = pooch.retrieve(\n    url=url,\n    known_hash=\"md5:39ee257f7eeb94cb60e6177cf80c9544\",\n)\n\ndf = pd.read_csv(file_path, delimiter=\" \")\n\n# the ground-truth dag is shown here: XXX: comment in when errors are fixed\nground_truth_dag = bnlearn.import_DAG(\"sachs\", verbose=False)\nfig = bnlearn.plot(ground_truth_dag)\n\n# .. note::\n#    The Sachs dataset has previously been preprocessed, and the steps are described\n#    in bnlearn, at the web-page https://www.bnlearn.com/research/sachs05/.\nprint(df.head())\nprint(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess the dataset\nSince the data is one dataframe, we need to process it into a form\nthat is acceptable by dodiscover's :class:`constraint.PsiFCI` algorithm. We\nwill form a list of separate dataframes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unique_ints = df[\"INT\"].unique()\n\n# get the list of intervention targets and list of dataframe associated with each intervention\nintervention_targets = [df.columns[idx] for idx in unique_ints]\ndata_cols = [col for col in df.columns if col != \"INT\"]\ndata = []\nfor interv_idx in unique_ints:\n    _data = df[df[\"INT\"] == interv_idx][data_cols]\n    data.append(_data)\n\nprint(len(data), len(intervention_targets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup constraint-based learner\nSince we have access to interventional data, the causal discovery algorithm\nwe will use that leverages CI and CD tests to estimate causal constraints\nis the Psi-FCI algorithm :footcite:`Jaber2020causal`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Our dataset is comprised of discrete valued data, so we will utilize the\n# G^2 (Chi-square) CI test.\nci_estimator = GSquareCITest(data_type=\"discrete\")\n\n# Since our data is entirely discrete, we can also use the G^2 test as our\n# CD test.\ncd_estimator = GSquareCITest(data_type=\"discrete\")\n\nalpha = 0.05\nlearner = PsiFCI(\n    ci_estimator=ci_estimator,\n    cd_estimator=cd_estimator,\n    alpha=alpha,\n    max_combinations=10,\n    max_cond_set_size=4,\n    n_jobs=-1,\n)\n\n# create context with information about the interventions\nctx_builder = make_context(create_using=InterventionalContextBuilder)\nctx: Context = (\n    ctx_builder.variables(data=data[0]).num_distributions(6).obs_distribution(False).build()\n)\n\nprint(ctx.init_graph)\nprint(ctx.f_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the learning process\nWe have setup our causal context and causal discovery learner, so we will now\nrun the algorithm using the :meth:`constraint.PsiFCI.learn_graph` API, which is similar\nto scikit-learn's `fit` design. All fitted attributes contain an underscore at the end.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learner = learner.learn_graph(data, ctx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze the results\nNow that we have learned the graph, we will show it here. Note differences and similarities\nto the ground-truth DAG that is \"assumed\". Moreover, note that this reproduces Supplementary\nFigure 8 in :footcite:`Jaber2020causal`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "est_pag = learner.graph_\n\nprint(f\"There are {len(est_pag.to_undirected().edges)} edges in the resulting PAG\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the full graph including the F-node\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dot_graph = draw(est_pag, direction=\"LR\")\ndot_graph.render(outfile=\"psi_pag_full.png\", view=True, cleanup=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the graph without the F-nodes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "est_pag_no_fnodes = est_pag.subgraph(ctx.get_non_augmented_nodes())\ndot_graph = draw(est_pag_no_fnodes, direction=\"LR\")\ndot_graph.render(outfile=\"psi_pag.png\", view=True, cleanup=True)\n\n# Interpretation\n# --------------\n# Looking at the supplemental figure 8b in :footcite:`Jaber2020causal`, we see that the\n# learned PAG matches quite well.\n\n# References\n# ----------\n# .. footbibliography::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}